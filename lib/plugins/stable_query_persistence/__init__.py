# Copyright (c) 2019 Charles University, Faculty of Arts,
#                    Department of Linguistics
# Copyright (c) 2019 Tomas Machalek <tomas.machalek@gmail.com>
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; version 2
# dated June, 1991.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
Stable concordance persistence provides a solution where KonText runs mainly as API and
it is desired to have always the same key/hash for the same input data. Although not required,
it is recommended to run this along with API-friendly auth plug-in 'static_auth'.

Keys generated by this plug-in can be read by default_conc_persistence (and ucnk_conc_persistence2).
So it should be possible to mix two instances of KonText - one with stable_conc_persistence and
one with default_conc_persistence on top of the same database and generate working links from
'stable_conc_persistence' instance to the 'default_conc_persistence' (the opposite direction is
not supported).
"""

import logging
import re
import time
import asyncio
import aiosqlite

import plugins
import ujson as json
from action.errors import ForbiddenException, NotFoundException
from plugin_types.general_storage import KeyValueStorage
from plugin_types.auth import AbstractAuth
from plugin_types.query_persistence import AbstractQueryPersistence
from plugin_types.query_persistence.common import (
    ID_KEY, QUERY_KEY, USER_ID_KEY, generate_idempotent_id)
from plugins import inject

DEFAULT_TTL_DAYS = 7


def mk_key(code):
    return 'concordance:%s' % (code, )


class StableQueryPersistence(AbstractQueryPersistence):

    def __init__(self, db: KeyValueStorage, auth: AbstractAuth, settings):
        super().__init__(settings)
        self.db = db
        self._auth = auth
        plugin_conf = settings.get('plugins', 'query_persistence')
        self._ttl_days = int(plugin_conf.get('ttl_days', DEFAULT_TTL_DAYS))
        self._archive_db_path = plugin_conf.get('archive_db_path')
        self._archive_lock = asyncio.Lock()
        self._settings = settings

    async def _mk_arch_table_if_none(self):
        async with self._archive_lock:
            async with aiosqlite.connect(self._archive_db_path) as db:
                async with db.execute(
                        "SELECT COUNT(*) FROM sqlite_master WHERE type = 'table' AND name = 'archive'") as cur:
                    ans = await cur.fetchone()
                    if ans[0] == 0:
                        logging.getLogger(__name__).warning('stable_conc_persistence archive table not present -creating one')
                        await db.execute(
                            'CREATE TABLE archive (id text, data text NOT NULL, created integer NOT NULL, num_access integer ' +
                            'NOT NULL DEFAULT 0, last_access integer, PRIMARY KEY (id))')

    @property
    def ttl(self):
        return self._ttl_days * 24 * 3600

    def is_valid_id(self, data_id):
        # we intentionally accept non-hex chars here, so we can accept also conc keys
        # generated from default_conc_persistence and derived plug-ins.
        return bool(re.match(r'~[0-9a-zA-Z]+', data_id))

    def get_conc_ttl_days(self, user_id):
        return self._ttl_days

    async def find_used_corpora(self, query_id):
        """
        Because the operations are chained via 'prev_id' and the corpname
        information is not stored for all the steps, for any n-th step > 1
        we have to go backwards and find an actual corpname stored in the
        1st operation.
        """
        data = await self._load_query(query_id, save_access=False)
        while data is not None and 'corpname' not in data:
            data = await self._load_query(data.get('prev_id', ''), save_access=False)
        return data.get('corpora', []) if data is not None else []

    async def open(self, data_id):
        ans = await self._load_query(data_id, save_access=True)
        if ans is not None and 'corpora' not in ans:
            ans['corpora'] = self.find_used_corpora(ans.get('prev_id'))
        return ans

    async def _load_query(self, data_id: str, save_access: bool):
        """
        Loads operation data according to the passed data_id argument.
        The data are assumed to be public (as are URL parameters of a query).

        arguments:
        data_id -- a unique ID of operation data

        returns:
        a dictionary containing operation data or None if nothing is found
        """
        data = await self.db.get(mk_key(data_id))
        if data is None and self._archive_db_path is not None:
            async with self._archive_lock:
                async with aiosqlite.connect(self._archive_db_path) as db:
                    db.row_factory = aiosqlite.Row
                    async with db.execute( 'SELECT data, num_access FROM archive WHERE id = ?', (data_id,)) as cur:
                        tmp = await cur.fetchone()
                        if tmp:
                            data = json.loads(tmp[0])
                    if save_access:
                        await db.execute(
                            'UPDATE archive SET last_access = ?, num_access = num_access + 1 WHERE id = ?',
                            (int(round(time.time())), data_id))
                        await db.commit()
        return data

    async def store(self, user_id, curr_data, prev_data=None):
        def records_differ(r1, r2):
            return (r1[QUERY_KEY] != r2[QUERY_KEY] or
                    r1.get('lines_groups') != r2.get('lines_groups'))

        if prev_data is None or records_differ(curr_data, prev_data):
            if prev_data is not None:
                curr_data['prev_id'] = prev_data[ID_KEY]
            data_id = generate_idempotent_id(curr_data)
            curr_data[ID_KEY] = data_id
            curr_data[USER_ID_KEY] = user_id
            data_key = mk_key(data_id)
            await self.db.set(data_key, curr_data)
            await self.db.set_ttl(data_key, self.ttl)
            latest_id = curr_data[ID_KEY]
        else:
            latest_id = prev_data[ID_KEY]

        return latest_id

    async def archive(self, conc_id, explicit):
        async with self._archive_lock:
            curr_conc_id = conc_id
            hard_limit = 100
            i = 0
            first_data = None
            async with aiosqlite.connect(self._archive_db_path) as db:
                data = await self.db.get(mk_key(curr_conc_id))
                while curr_conc_id is not None and i < hard_limit:  # hard_limit prevents ending up in infinite loops of 'prev_id'
                    if i == 0:
                        first_data = data
                    if data is None:
                        raise NotFoundException('Concordance {0} not found'.format(conc_id))
                    curr_time = time.time()
                    await db.execute(
                        'INSERT OR IGNORE INTO archive (id, data, created, num_access) VALUES (?, ?, ?, ?)',
                        (conc_id, json.dumps(data), curr_time, 0))
                    await db.commit()
                    curr_conc_id = data.get('prev_id', None)
            return first_data


    async def update(self, data):
        """
        Update stored data by data['id']. Used only for internal data correction!
        """
        data_id = data[ID_KEY]
        data_key = mk_key(data_id)
        if await self.db.exists(data_key):
            await self.db.set(data_key, data)

        async with self._archive_lock:
            async with aiosqlite.connect(self._archive_db_path) as db:
                await db.execute('UPDATE archive SET data = ? WHERE id = ?', (json.dumps(data), data_id,))
                await db.commit()

    async def clone_with_id(self, old_id: str, new_id: str):
        """
        Duplicate entry with new id
        """
        # check if new id is available
        if await self.id_exists(new_id):
            raise ValueError(f'ID {new_id} already exists')

        # get original data
        original_data = await self.db.get(mk_key(old_id))
        if original_data is None:
            async with self._archive_lock:
                async with aiosqlite.connect(self._archive_db_path) as db:
                    async with db.execute(
                            'SELECT data FROM archive WHERE id = ? LIMIT 1',
                            (old_id,)) as cursor:
                        row = await cursor.fetchone()
                        if row is None:
                            raise ValueError(f'Data for {old_id} not found')
                        original_data = json.loads(row[0])
        # set new values
        original_data[ID_KEY] = new_id
        await self.db.set(mk_key(new_id), original_data)

    async def id_exists(self, id: str) -> bool:
        """
        Check if ID already exists
        """
        ex = await self.db.exists(mk_key(id))
        if ex:
            return True
        async with self._archive_lock:
            async with aiosqlite.connect(self._archive_db_path) as db:
                async with db.execute(
                        'SELECT COUNT(*) FROM archive WHERE id = ? LIMIT 1',
                        (id,)) as cursor:
                    row = await cursor.fetchone()
                    return row[0] > 0

    async def on_init(self):
        await self._mk_arch_table_if_none()


@inject(plugins.runtime.DB, plugins.runtime.AUTH)
def create_instance(settings, db: KeyValueStorage, auth: AbstractAuth):
    return StableQueryPersistence(db=db, auth=auth, settings=settings)
